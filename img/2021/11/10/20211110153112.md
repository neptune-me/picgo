## 3

主要贡献

提出了残差学习框架ResNet，解决了深网络难以训练的问题。

方法

<img src="D:\data\Typora_picture\image-20211110113351460-16365152478791.png" alt="image-20211110113351460" style="zoom:50%;" />

引入残差模块。对于stacked layers，学习到的特征是H(x)，现在希望学习到的是残差H(x) - x，记为F(x)。这样原始的特征就是F(x) + x。

残差学习相比原始学习更容易，因为当这一层做恒等变换时，学习到的残差就是0，这保证了深网络的性能至少不会下降。又因为残差其实不会为0，堆积层可以在输入特征基础上学习到新的特征，从而获得更好的性能。

残差学习单元的结构是短路连接，增加了一条输入x到输出的短路连接，在论文中是恒等变换。

**网络结构**

34层resnet如下图，由1个卷积层+16个block+1个全连接组成，一个block由两个conv层组成。

虚线代表输入和输出维度不一致时，对x进行一定处理后再与输出相加。（两种处理：1.  zero padding 2. projection shortcut，短路连接上采用1*1卷积升维或降维）

![img](https://pic2.zhimg.com/v2-7cb9c03871ab1faa7ca23199ac403bd9_b.jpg)

**bottleneck**

<img src="https://img-blog.csdnimg.cn/20190412171201596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1ZXlhbmc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

bottleneck层

**实验结果**

![img](https://pic2.zhimg.com/v2-ac88d9e118e3a85922188daba84f7efd_b.jpg)

解决了传统网络的退化问题，层数越深准确率越高。

## 4

### 主要贡献

提出了一种将每一层与前面的所有层连接的网络DenseNet

### 方法

网络结构如下图所示：



Conv - Pooling - {DenseBlock - Transition Layer} * n

DenseBlock: DenseBlock由多层组成，论文中的第一张图是一个5层的Block，其中每一层都与preceding layers相连接，连接的方式是将每一层的输入concate成为一个tensor。DenseBlock又由多个{bottleneck层-卷积层}连接而成。

Bottle neck层：1x1卷积层，用于减少特征图**数量**

Transition Layer: 由卷积和池化层组成，可以改变特征图的**大小**，用于连接相邻的Dense Block。论文中的Transition Layer能把特征图大小变为原来的1/2，由1x1卷积层和2x2平均池化层组成。

Compact：用于减少Transition Layer的特征图数量。若一个Dense Block产生m个特征图，那它接下来的transition layer就生成θm个特征图。



### 细节

H(·)：每一层的输入经过这个H函数变换得到输出，H是多个函数的复合（affine bn relu）。·是{x0,x1,...,xl}的缩写，意思是

Growth Rate k ：H(·)产生的特征图的数量。一个DenseBlock的第l层的输入特征图数量计算公式为k0+(l-1)×k。



### 结果

比ResNet还要好。与它的不同之处在于连接preceding layers的方式由原来的简单加和summation 变成了 concatenation



## 5

参数量是原来的1/N，N是输入通道数。

resolution  multiplier ρ：作用在输入特征图，使输入特征图变为ρ长*ρ宽。减少计算量

### 实验结果

计算量和准确率有一个log的关系。分辨率和准确率有一个正比的关系。