# Lab1 Part1

## 拟合任务

使用三层网络+50神经元的网络结构，红色的曲线为拟合所得曲线，蓝色的曲线为真实的sin x曲线。输入(-pi/2,pi/2)，步长为0.01的数据进行测试，平均误差为-0.00019。

<img src="D:\data\Typora_picture\拟合.png" alt="拟合"  />

使用了与回归任务不同的损失函数，拟合任务使用的是mean squared loss，而分类任务使用了cross entropy。造成损失函数选择差异的原因在于两种任务的目标差异。拟合任务的目标在于预测值与实际值越接近越好。而分类任务目标是已知各样本的正确类别，将预测正确的概率最大化。而cross entropy是由最大化似然函数这个优化问题推导得出的。

拟合问题不能用cross entropy，因为拟合问题没有ground truth。

分类问题，最后必须是 one hot 形式算出各 label 的概率， 然后通过 argmax 选出最终的分类。如果用 MSE 计算 loss， 输出的曲线是波动的，有很多局部的极值点。 即，非凸优化问题。而cross entropy 计算 loss，则依旧是一个凸优化问题，用梯度下降求解时，凸优化问题有很好的收敛特性，模型不会陷入局部最小值导致收敛到一个很差的值。

所以拟合任务适合用mse做损失函数，不能用cross entropy。分类任务更适合用cross entropy。

## 代码结构

![image-20211016134808615](D:\data\Typora_picture\image-20211016134808615.png)

layers.py: 定义各种形式的前向传播、反向传播、损失函数

layer_util.py:组装各个层次（比如说考虑不考虑batchnorm等）的各种网络前向反向传播大模块

fc_net.py:将整个网络组装好（主要是要初始化参数、定义损失函数）

solver.py:训练、保存网络。可以根据不同的更新规则（比如sgd）执行不同的梯度下降；接受训练集和验证集及它们的标签，周期性地检查准确率等等。

optim.py: 定义了各种优化器（sgd、adam）

## 不同网络结构

### 测试不同层数、神经元数

| 设定              | 最高验证准确率 |
| ----------------- | -------------- |
| 2层网络+10神经元  | 81.0%          |
| 2层网络+50神经元  | 90.1%          |
| 2层网络+100神经元 | 92.4%          |
| 3层网络+10神经元  | 82.0%          |
| 3层网络+50神经元  | 91.0%          |
| 3层网络+100神经元 | **92.7%        |
| 4层网络+ 10神经元 | 83.2%          |
| 4层网络+50神经元  | 91.2%          |
| 4层网络+100神经元 | 92.2%          |

这一组实验旨在探究网络结构对实验的影响。固定网络层数，可以发现增加神经元数使最高验证准确率上升；固定神经元数，发现增加网络层数使最高验证准确率上升，但是这个增加的幅度很小。这个结果令人很疑惑，与前人的经验之谈相违背——与在每一层中添加更多的神经元相比，**添加层层数将获得更大的性能提升**。而我的实验结果似乎与之相反。经过思考得知，原因在于实验设计并不合理。第一个问题在于，神经元数增加跨度太大。一层网络的神经元为k，训练网络的时间复杂度为O(k^2)。实验中由10个神经元增加到50个神经元，网络的计算量扩大了上百倍，自然性能有了一个大跃升；第二个问题在于，网络层数增加幅度太小。神经元扩大一倍，对应相同复杂度的更深的网络，相当于是4倍深度。神经元数量从10增加到20带来的提升，应该与网络层数从2增加到8一致。所以重新设计实验，如下图所示：

| 网络层数\神经元数量 | 10    | 20    | 30    | 50    | 100   |
| ------------------- | ----- | ----- | ----- | ----- | ----- |
| 2                   | 81.0% | 84.6% | 87.6% | 90.1% | 92.4% |
| 4                   | 83.2% | 86.4% | 89.0% | 90.7% | 92.7% |
| 8                   | 83.2% | 88.6% | 88.8% | 90.5% | 96.8% |
| 16                  | 90.0% | 94.2% | 95.3% | 96.0% | 96.3% |

实验结果显示，网络层数从2层增加至16层（8倍）带来的准确率提升，比神经元数从10增加到30个（9倍），要更大。因此，**不需要在一个隐藏层中加入过多的神经元**。究其原因有二。首先，隐藏层中的神经元过多可能会导致**过拟合(overfitting)**。当神经网络具有过多的节点（过多的信息处理能力）时，训练集中包含的有限信息量不足以训练隐藏层中的所有神经元，因此就会导致过拟合。即使训练数据包含的信息量足够，隐藏层中过多的神经元会增加训练时间，从而难以达到预期的效果。显然，选择一个合适的隐藏层神经元数量是至关重要的。

另外，网络层数也要选择恰当，8层100神经元的准确率比16层100神经元的准确率更高，可以推测16层100神经元的模型发生了过拟合。所以不能不考虑实际而直接无脑堆砌多层神经网络。

## 网络参数实验比较

以下的实验为控制变量，将固定网络结构为3层网络50神经元，每组不同的实验保证只有一个网络参数不一样。

以下的实验结果均为训练到loss收敛后的结果，此时网络基本过拟合到99%+的训练准确率。

### 学习率LR

| 设定                | 最高验证准确率 |
| ------------------- | -------------- |
| 常数学习率，LR=3e-2 | 89.0%          |
| 常数学习率，LR=1e-2 | 90.1%          |
| 常数学习率，LR=3e-3 | 90.0%          |
| 常数学习率，LR=1e-3 | 86.4%          |
| 常数学习率，LR=6e-3 | **91.5%        |
| 常数学习率，LR=1e-1 | 70.5%          |

学习率3e-3时的训练速度较快，准确率最高。

本组实验探究学习率对模型性能的影响。学习率是影响模型训练时长、最终性能最重要的超参数，它决定了参数一步更新的大小。如果学习率较小，则参数更新慢，甚至无法更新； 反之如果学习率较大，则参数更新快，但可能会无法学习到细节信息并在最终准确率附近不断以较大的程度上下波动，导致训练不稳定甚至无法收敛。

实验尝试了常数学习率，发现当学习率从 0.1 开始下降到 0.0001 的过程中，最高验证准确率先上升后下降。学习率为6e-3时，收敛速度最快，训练效果最好。

<img src="D:\data\Typora_picture\image-20211016142503161.png" alt="image-20211016142503161" style="zoom:50%;" />

### 正则化项reg

| 设定       | 最高验证准确率 |
| ---------- | -------------- |
| reg = 1e-1 | 90.9%          |
| reg = 1e-2 | 90.0%          |
| reg = 1e-3 | 86.6%          |
| reg = 0    | 86.7%          |

L2 正则化是一种有效的正则化方法，能够明显地提升模型的泛化能力。这种方法通过在损失函数上添加参数的范数项，使得模型训练有缩小参数的倾向，最终得到一个参数绝对值较小的模型。在奥卡姆剃刀原则的假设下，其他条件相同时我们应该更加相信那个更简单的解释。于是我们也可以想象在训练准确率相同的情况下，我们应该更相信那个参数更小的模型。正则化对于模型参数缩小的影响可以通过调节正则化参数调节，这个参数越大，模型更加倾向于使得参数缩小。由于交叉熵损失函数是分类任务中几乎唯一的合适选择，于是通过改变正则化参数来实验不同的损失函数对模型的影响。

实验结果显示正则化项设为1e-1和1e-2时，最高验证正确率都达到了90%，且训练正确率也不再是惊人的100%，说明正则化项对改善模型过拟合的效果是很显著的。

### batch size

| 设定             | 最高验证准确率 | 用时s |
| ---------------- | -------------- | ----- |
| batch size = 16  | 89.7%          | 125   |
| batch size = 32  | **91.1%        | 180   |
| batch size = 64  | 90.9%          | 113   |
| batch size = 128 | 89.2%          | 88    |
| batch size = 256 | 86.6%          | 119   |

在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率。在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。

批大小（batch size）可以被视为最重要的超参数之一，它直接决定了一次参数更新迭 代由多少不同的样本组成的样本空间决定。而它的这个特点又间接影响了训练收敛需要的迭代次数以及最终的模型精度。经验之谈，一般批大小设置得越小，达到相同精度所需要的迭代次数越多，Epoch（一个 Epoch 为遍历一次所有样本）越少，最终的训练时间越长。而能 够达到最佳精度的批大小普遍出现在 128 左右，故实验设计为由 128 为中央分界线，往两侧 寻找最佳的参数。经过实验我们发现最佳的批大小为 32，此时最高验证准确率达到最大值。

在此基础上再增加batch size的效果不增反降，这个结果是反直觉的，因为更大的 batch size 更能准确地计算出梯度，mini-batch 只是用更少的计算来近似梯度。直觉上好像更大的 batch 更好因为更“准确”。但是batch大、梯度准确会导致模型陷入局部最小值出不去了，从而导致模型收敛到一个很差的点，拟合能力很差。

但是，大的 batch size也是有优点的。 在并行计算上更占优势，太小的 batch-size 发挥不了 gpu 的性能。我们的实验也显示在一定范围内增大batch size，训练同样的epoch时长减少。所以当我们觉得 batch 要更大，主要是为了更好的并行，而不是为了更精确的梯度。

### 激活函数

| 设定              | 最高验证准确率 |
| ----------------- | -------------- |
| 所有层使用Sigmoid | 86.6%          |
| 所有层使用ReLU    | 91.0%          |
| 所有层使用tanh    | 88.4%          |

在提取特征的网络层中，对照组使用的是 ReLU 激活函数。还尝试了2种不同的激活函数，它们是软饱和激活函数 Sigmoid, Tanh。实验结果显示ReLU的效果最佳，训练同样的epoch的情况下收敛速度最快。其次是tanh，收敛速度最慢、准确率最低的是sigmoid。tanh比sigmoid更快收敛，是因为sigmoid输出总为正数，优化路径容易出现zigzag现象，而以0为中心的tanh函数则不会出现此现象。而relu比tanh和sigmoid表现更好，原因在于sigmoid和tanh激活函数有共同的缺点——即在输入数据很大或很小时，梯度几乎为零，因此使用梯度下降优化算法更新网络很慢。

下图从左至右依次是sigmoid, relu, tanh作为激活函数时，training_loss的变化曲线。

<center class="half">    <img src="D:\data\Typora_picture\image-20211013214318741.png" width="200"/>    <img src="D:\data\Typora_picture\image-20211013214759099.png" width="200"/> <img src="D:\tanh.png" width="200"/></center>







## 优化训练和网络的方法

### batch normalization

Batch Normalization，批标准化，和普通的数据标准化类似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法。

在神经网络中，数据分布对训练会产生影响。 比如某个神经元 x 的值为1，某个 Weights 的初始值为 0.1，这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20，这样 Wx 的结果就为 2。 当我们加上一层激活函数, 激活这个 Wx 值的时候，问题来了。如果使用像 sigmoid 或者 tanh 的激活函数，Wx 的激活值就变成了 -0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大，激活函数输出值也还是 接近1。换句话说，神经网络在初始阶段已经不对那些比较大的 x 特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。当然我们是可以用对数据做 normalization 预处理，使得输入的 x 变化范围不会太大，让输入值经过激活函数的敏感部分。但这个不敏感问题不仅仅发生在神经网络的输入层，在隐藏层中也同样会发生。所以我们需要对每一层输入的数据进行标准化来解决梯度消失的问题。

Batch normalization也可以被看做网络中的一个层。在一层层的添加神经网络的时候，我们将数据 X输入到全连接层，全连接层的计算结果会经过激活函数的作用成为下一层的输入，接着重复之前的操作。Batch Normalization (BN) 就被添加在每一个全连接层和激活函数之间。具体实现如下图伪代码所示：

![img](https://img-blog.csdn.net/20170425131212885?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhhbmd4YjM1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![image-20211014135817314](D:\data\Typora_picture\image-20211014135817314.png)

Batch Normalization的效果是很显著的，不使用BN的网络准确率只有28.4%，而使用BN后准确率提升到了90.6%。

### data augmentation

众所周知，深度学习模型本身对最终性能的影响固然巨大，但是影响更为广泛的是数据。数据的数量以及质量决定了不论使用什么模型能达到的性能上限。试想如果我们不让一个小孩接触足够高质量的教 育、足够多的练习机会，又怎么能够期盼他成为社会的顶梁柱、某个领域的专家呢？方仲永 就是一个很好的例子，即使天资再高（模型足够精妙），不接受足够的良好教育（接受的数 据样本少，且质量低），必定是无法成才的。这是由于复杂的现实使得例外样本在总样本中 占比不低，数据分布事实上是呈现长尾性质的。常见的几类样本在真实数据中占据大多数， 而余下的成千上万种数据却不常见，如果没有良好的举一反三的能力（泛化能力）自然一旦遇到偏怪数据时就会显得无能为力。所以我们需要模型的决策边界尽量平滑且覆盖所有情况，以达到遇见不常见数据时不会过于固执（过拟合）的效果。

在汉字分类的任务中，由于数据是汉字的黑白图像，且图像尺寸很小，所以排除了经典的数据增强方法——随机灰度和random erasing。另外根据文字的特点，大多数汉字在90°及以上旋转后将无法辨认其意义，所以最后选用了随机的平移和微小旋转结合的方法。最高验证准确率提升到97%，且训练准确率。代码实现如下图：

<img src="D:\data\Typora_picture\image-20211016142747668.png" alt="image-20211016142747668" style="zoom:50%;" />

## 对反向传播算法的理解

反向传播算法的反向体现在与前向传播的“前向”方向相反，即前向传播是将**输入数据**沿着方向input - {affine - batchnorm - relu} * N - affine - output逐层传播，到反向传播则是将**梯度**沿着方向output - affine - {relu - batchnorm - affine} * N - input进行传播。而传到不同作用的层，所进行的操作都是为了达成反向传播的目标——就是要将损失函数最小化，所以在relu层的反向传播，就是计算损失函数对x的梯度；在batch norm层计算损失函数对x、gamma、beta的梯度；在affine层计算损失函数对x、w、b的梯度。接着根据梯度下降的更新规则——更新沿着损失函数梯度的反方向，将上述参数进行更新。

